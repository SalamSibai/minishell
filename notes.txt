1- scan inputs : to identifie the tokens 
2- check for validitiy - clean
3- create the abstract syntax tree


cases:
    1- redirecting to read from the standard input (heredoc)'<<'
        - next input must be a limiter
    2- reading from an infile
        - input must be a filename 
    3- redirecting input to a specific file '<'
        - next input must be a filename
    4- redirects output to a specific file '>'
        - next input must be a filename
    5- redirects output to append mode '>>'
        - next input must be a filename

ls -l | wc -l
	ls -l > file
	ls -l >> file
	ls -l < file
	ls -l << limiter

	ls -l | wc -l | ls -l > file
	ls -l | wc -l | ls -l >> file
/* examples: 
	ls -l | wc -l
	ls -l > file
	ls -l >> file
	ls -l < file
	ls -l << limiter

	ls -l | wc -l | ls -l > file
	ls -l | wc -l | ls -l >> file
	echo hisham > f2 9 8 7 >> ss 77 66 << ju | < mn cat 5 4 3 
	


	/*
		execution flow:
			1- create t_data struct and initialize
				- split the PATH into the **path variable
				- store env in **envp
				- initialize and set t_env variable
			2- malloc for and initialize parse_data
			3- scan and fill in lexer & tokens
				- check the tokens, sets the type, and the string that represents the token.
				- increments the lexer (based on the token type)
				- this function also cleans up the string, making sure there are no spaces etc
			4- start filling in **cmd variable from the tokens
			5- if we have pipes:
				- initialzie and set the *pipe varibale from the lexer and the tokenizer
			6- if we have redirections:
				- "> outfile_name" we check the syntax correctness (has a filename or limiter after)
				- if everything is fine, we initialize and store
	*/